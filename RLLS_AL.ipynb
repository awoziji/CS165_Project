{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This program combines the importance weight estimation w from RLLS (refer to Anqi Liu)\n",
    "# and then combine with Active Learning strategy to save label cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from mnist_for_labelshift import MNIST_SHIFT\n",
    "from cifar10_for_labelshift import CIFAR10_SHIFT\n",
    "import torchvision\n",
    "from resnet import *\n",
    "import cvxpy as cp\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 1000\n",
    "use_cuda = 0 # we use cpu\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "iterations = 1\n",
    "sigma = 0\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "shift_type = 2\n",
    "shift_para = 0.2\n",
    "shift_para_aux = None\n",
    "#model = 'MLP'\n",
    "epochs_estimation = 1\n",
    "epochs_training = 1\n",
    "epochs_validation = 1\n",
    "labda = [1.0]\n",
    "data_name = 'mnist'\n",
    "n_class = 10\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test 1\n",
    "# training_size = 30000\n",
    "# testing_size = 15000\n",
    "# testsize_range = [5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 20000\n",
    "testing_size = 15000\n",
    "testsize_range = [5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.D_in = D_in\n",
    "        self.H = H\n",
    "        self.D_out = D_out\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.D_in, self.H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.H, self.D_out),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.D_in)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, weight=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #print(\"train(): batch_idx = \", batch_idx, \"; len(data) = \", len(data))\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #print(\"output type: \", type(output))\n",
    "        if weight is None:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss(weight)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # ???\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_al(model, device, train_loader, extra_data, optimizer, epoch, weight):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #print(\"train(): batch_idx = \", batch_idx, \"; len(data) = \", len(data))\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        criterion = nn.CrossEntropyLoss(weight)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    for data, target in extra_data:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output, target)\n",
    "        #print(\"loss = \", loss, \", loss type = \", type(loss) )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, 0 * len(data), len(extra_data),\n",
    "                100. * 0 / len(extra_data), loss.item()))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, weight=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    prediction = np.empty([0,1])\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            #print(\"output type: \", type(output))\n",
    "            if weight is None:\n",
    "                criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "            else:\n",
    "                criterion = nn.CrossEntropyLoss(weight, reduction='sum')\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()# sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            pred = pred.cpu().numpy()\n",
    "            prediction = np.concatenate((prediction, pred))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.5f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),float(100. * correct / len(test_loader.dataset))))\n",
    "    return prediction, float(100. * correct / len(test_loader.dataset)), test_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_query(model, device, test_loader, query_size):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    prediction = np.empty([0,1])\n",
    "    \n",
    "    probs_list = []\n",
    "    uncertainty_queue = []\n",
    "    \n",
    "    id_data_target = {}\n",
    "    id_data = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            #print(\"output type: \", type(output))\n",
    "            criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()# sum up batch loss\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            probs = output.max(1, keepdim=True)[0]\n",
    "            \n",
    "            #print(\"size of probs tensor = \", probs.size())\n",
    "            #print(\"probs.cpu().numpy() = \", probs.cpu().numpy())\n",
    "            sum_output_tensor = torch.sum(output, 1)\n",
    "            sum_output = sum_output_tensor.cpu().numpy()[0]\n",
    "            \n",
    "            prob = probs.cpu().numpy()[0][0] / sum_output\n",
    "            \n",
    "            heapq.heappush(probs_list, (prob, id_data))\n",
    "            id_data_target[id_data] = (data, target)\n",
    "            id_data += 1\n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            pred = pred.cpu().numpy()\n",
    "            prediction = np.concatenate((prediction, pred))\n",
    "    \n",
    "    for i in range(query_size):\n",
    "        prob, id_data = heapq.heappop(probs_list)\n",
    "        uncertainty_queue.append(id_data_target[id_data])\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Average loss: {:.4f}, Accuracy: {}/{} ({:.5f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),float(100. * correct / len(test_loader.dataset))))\n",
    "    return prediction, 100. * correct / len(test_loader.dataset), test_loss, uncertainty_queue\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute weight for Q(y)/P(y) = w\n",
    "def compute_w_opt(C_yy,mu_y,mu_train_y, rho, labda = 1):\n",
    "\n",
    "    n = C_yy.shape[0]\n",
    "    theta = cp.Variable(n)\n",
    "    b = mu_y - mu_train_y\n",
    "    objective = cp.Minimize(cp.pnorm(C_yy*theta - b) + rho* cp.pnorm(theta))\n",
    "    constraints = [-1 <= theta]\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "\n",
    "    result = prob.solve()\n",
    "    w = 1 + theta.value * labda\n",
    "\n",
    "    print('Estimated w is', w)\n",
    "   \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_true_w(train_labels, test_labels, n_class, m_train, m_test):\n",
    "     # compute the true w\n",
    "    mu_y_train = np.zeros(n_class)\n",
    "    for i in range(n_class):\n",
    "        mu_y_train[i] = float(len(np.where(train_labels == i)[0]))/m_train\n",
    "    mu_y_test = np.zeros(n_class)\n",
    "    for i in range(n_class):\n",
    "        mu_y_test[i] = float(len(np.where(test_labels == i)[0]))/m_test\n",
    "    true_w = mu_y_test/mu_y_train\n",
    "    print('True w is', true_w)\n",
    "    return true_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_3deltaC(n_class, n_train, delta):\n",
    "    rho = 3*(2*np.log(2*n_class/delta)/(3*n_train) + np.sqrt(2*np.log(2*n_class/delta)/n_train))\n",
    "    return rho "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_alpha(n_class, C_yy, mu_y, mu_y_train, rho, true_w):\n",
    "    alpha = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
    "    w2 = np.zeros((len(alpha), n_class))\n",
    "    for i in range(len(alpha)):\n",
    "\n",
    "        w2[i, :] = compute_w_opt(C_yy, mu_y, mu_y_train, alpha[i] * rho)\n",
    "    mse2 = np.sum(np.square(np.matlib.repmat(true_w, len(alpha),1) - w2), 1)/n_class\n",
    "    i = np.argmin(mse2)\n",
    "    print(\"choose_alpha - mse2, \", mse2)\n",
    "    return alpha[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_perclass(y, predictions, n_class):\n",
    "\n",
    "    acc = np.zeros(n_class)\n",
    "    predictions = np.concatenate(predictions)\n",
    "\n",
    "    for i in range(n_class):\n",
    "        si = float(len(np.where(y == i)[0]))\n",
    "        if si != 0:\n",
    "            acc[i] = float(len(np.where((predictions == i)& (y == i))[0]))/float(len(np.where(y == i)[0]))\n",
    "        else:\n",
    "            acc[i] = 0\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test(device, use_cuda, w, train_model, init_state, train_loader, test_loader, validate_loader, test_labels, n_class):\n",
    "    w = torch.tensor(w)\n",
    "    train_model.load_state_dict(init_state)\n",
    "    if use_cuda:\n",
    "        w = w.cuda().float()\n",
    "        train_model.cuda()\n",
    "    else:\n",
    "        w = w.float()\n",
    "    \n",
    "    best_loss = 10\n",
    "    # model = train_model.to(device)#ConvNet().to(device)\n",
    "    print(\"train_validate_test --- best_loss = \", best_loss)\n",
    "    optimizer = optim.SGD(train_model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "    for epoch in range(1, epochs_training + 1):\n",
    "        print(\"i am in for loop for train_validate_test!!!\")\n",
    "        train(train_model, device, train_loader, optimizer, epoch, weight=w) \n",
    "        # save checkpoint\n",
    "        \n",
    "        if epoch >= epochs_validation:\n",
    "            # validation\n",
    "            _, _, loss = test(train_model, device, validate_loader, weight=w)\n",
    "            print(\"saving model ---- loss = \", loss)\n",
    "            if loss < best_loss: \n",
    "                print('saving model')\n",
    "                state = {\n",
    "                    'model': train_model.state_dict(),\n",
    "                    }\n",
    "                if not os.path.isdir('checkpoint'):\n",
    "                    os.mkdir('checkpoint')\n",
    "                torch.save(state, './checkpoint/ckpt.pt')\n",
    "                best_loss = loss\n",
    "        \n",
    "    print('\\nTesting on test set')\n",
    "    # read checkpoint\n",
    "    print('Reading model')\n",
    "    checkpoint = torch.load('./checkpoint/ckpt.pt')\n",
    "    train_model.load_state_dict(checkpoint['model'])\n",
    "    predictions, acc, _ = test(train_model, device, test_loader)\n",
    "    f1 = f1_score(test_labels, predictions, average='macro')\n",
    "    f2 = f1_score(test_labels, predictions, average='micro') \n",
    "    acc_per_class = acc_perclass(test_labels, predictions, n_class)\n",
    "     \n",
    "    print('F1-score-micro:', f2)\n",
    "    print('F1-score-macro:', f1)\n",
    "    return acc, f1, f2, acc_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift-type: 2\n",
      "shift parameters:  0.2\n",
      "num_paras =  1\n",
      "testsize_range =  [5000]\n",
      "num_labda =  1\n",
      "labda =  [1.0]\n"
     ]
    }
   ],
   "source": [
    "print('Shift-type:', shift_type)\n",
    "print('shift parameters: ', shift_para)\n",
    "#shift type\n",
    "\n",
    "\n",
    "num_paras = len(testsize_range)\n",
    "print(\"num_paras = \", num_paras)\n",
    "print(\"testsize_range = \", testsize_range)\n",
    "num_labda = len(labda)\n",
    "print(\"num_labda = \", num_labda)\n",
    "print(\"labda = \", labda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Size =  35000\n",
      "Test Data size =  15000\n",
      "Training Data size =  20000\n",
      "\n",
      "Training using training_data1, testing on training_data2 to estimate weights.\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 2.356254\n",
      "Finish training for h_0\n"
     ]
    }
   ],
   "source": [
    "# train h_0 \n",
    "\n",
    "# Download data and separate into training and test\n",
    "# load raw_data\n",
    "# raw_data = MNIST_SHIFT('data/mnist', training_size, training_size, 1, sigma, target_label=2, train=True, download=True, \\\n",
    "#             transform=transforms.Compose([ \\\n",
    "#                            transforms.ToTensor(), \\\n",
    "#                            transforms.Normalize((0.1307,), (0.3081,)) \\\n",
    "#                            ]))\n",
    "raw_data_h0 = MNIST_SHIFT('data/mnist', training_size, testing_size, 1, sigma, target_label=2, train=True, download=True, \\\n",
    "            transform=transforms.Compose([ \\\n",
    "                           transforms.ToTensor(), \\\n",
    "                           transforms.Normalize((0.1307,), (0.3081,)) \\\n",
    "                           ]))\n",
    "\n",
    "# saparate into training and testing\n",
    "m_h0 = len(raw_data_h0)\n",
    "print(\"Raw Data Size = \", m_h0)\n",
    "\n",
    "m_test_h0 = raw_data_h0.get_testsize()\n",
    "test_indices_h0 = range(m_test_h0)\n",
    "test_data_h0 = data.Subset(raw_data_h0, test_indices_h0) # type dataset\n",
    "print('Test Data size = ', m_test_h0)\n",
    "\n",
    "m_train_h0 = m_h0 -  m_test_h0\n",
    "train_data_h0 = data.Subset(raw_data_h0, range(m_test_h0, m_h0)) # type dataset\n",
    "print('Training Data size = ', m_train_h0)\n",
    "\n",
    "# get labels for future use\n",
    "test_labels_h0 = raw_data_h0.get_test_label() # numpy array\n",
    "train_labels_h0 = raw_data_h0.get_train_label() # numpy array\n",
    "\n",
    "# base model for h_0\n",
    "D_in = 784\n",
    "base_model = Net(D_in, 256, 10)\n",
    "base_model = base_model.to(device)\n",
    "# saparate into training and validation\n",
    "# finish data preprocessing\n",
    "# estimate weights using training and validation set\n",
    "train_loader_h0 = data.DataLoader(train_data_h0, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "#model = ResNet18(**kwargs).to(device)#ConvNet().to(device)\n",
    "optimizer = optim.SGD(base_model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "print('\\nTraining using training_data1, testing on training_data2 to estimate weights.') \n",
    "for epoch in range(1, epochs_estimation + 1):\n",
    "    train(base_model, device, train_loader_h0, optimizer, epoch)\n",
    "print(\"Finish training for h_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data Size 35000\n",
      "All Test Size, 15000\n",
      "All Train Size,  20000\n",
      "test_size_range =  5000\n",
      "test_labels shape =  (5000,)\n",
      "train_labels shape =  (20000,)\n",
      "\n",
      "Testing on training_data2 to estimate C_yy.\n",
      "Average loss: 0.3880, Accuracy: 17780/20000 (88.90000%)\n",
      "\n",
      "\n",
      "Testing on test data to estimate mu_y.\n",
      "Average loss: 0.3744, Accuracy: 4452/5000 (89.04000%)\n",
      "\n",
      "Estimated w is [1.04082098 1.28267475 0.49072941 1.08733626 1.08435536 1.14561165\n",
      " 1.13243625 1.15993498 1.08858884 1.11248658]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Our weight:  [1.04082098 1.28267475 0.49072941 1.08733626 1.08435536 1.14561165\n",
      " 1.13243625 1.15993498 1.08858884 1.11248658]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "True w is [1.13363029 1.25636672 0.49188325 1.13322091 1.09420692 1.02867784\n",
      " 1.11723879 1.13106525 1.10644734 1.13081122]\n",
      "Mean square error of (true_w, w2) =  0.0026902123403365315\n"
     ]
    }
   ],
   "source": [
    "# calculate importance weights\n",
    "\n",
    "tw_tensor = torch.zeros([num_paras, 10])\n",
    "# assuming training size >> testing size\n",
    "#raw_data = MNIST_SHIFT('data/mnist', training_size, training_size, shift_type, shift_para, parameter_aux = shift_para_aux,target_label=2, train=True, download=True,\n",
    "raw_data = MNIST_SHIFT('data/mnist', training_size, testing_size, shift_type, shift_para, parameter_aux = shift_para_aux,target_label=2, train=True, download=True, \n",
    "        transform=transforms.Compose([ \n",
    "                   transforms.ToTensor(), \n",
    "                   transforms.Normalize((0.1307,), (0.3081,)) \n",
    "                   ]))\n",
    "# model for importance weights\n",
    "D_in = 784\n",
    "train_model_w = Net(D_in, 256, 10)\n",
    "train_model_w = train_model_w.to(device)\n",
    "init_state = copy.deepcopy(train_model_w.state_dict())\n",
    "\n",
    "# saparate into training and testing\n",
    "m = len(raw_data)\n",
    "print(\"All Data Size\", m)\n",
    "\n",
    "m_test = raw_data.get_testsize()\n",
    "print('All Test Size,', m_test)\n",
    "test_indices = range(m_test)\n",
    "\n",
    "m_train = m -  m_test\n",
    "print('All Train Size, ', m_train)\n",
    "\n",
    "range_m_test = testsize_range[0]\n",
    "print('test_size_range = ', range_m_test)\n",
    "\n",
    "range_test_data = data.Subset(raw_data, test_indices[0 : testsize_range[0]]) # 4000 test samples from all test data\n",
    "train_data = data.Subset(raw_data, range(m_test, m))\n",
    "\n",
    "# get labels for future use\n",
    "range_test_labels = raw_data.get_test_label()[0 : testsize_range[0]] # 4000 test labels from all test data\n",
    "print(\"test_labels shape = \", range_test_labels.shape)\n",
    "\n",
    "train_labels = raw_data.get_train_label() # labels of all train data\n",
    "print(\"train_labels shape = \", train_labels.shape)\n",
    "\n",
    "# finish data preprocessing\n",
    "# estimate weights using training and validation set\n",
    "test_loader_train_data = data.DataLoader(train_data, batch_size=batch_size, shuffle=False, **kwargs) # all train data\n",
    "print('\\nTesting on training_data2 to estimate C_yy.')\n",
    "predictions, acc, _ = test(base_model, device, test_loader_train_data)\n",
    "# compute C_yy \n",
    "C_yy = np.zeros((n_class, n_class)) \n",
    "#print(m_train_v)\n",
    "predictions = np.concatenate(predictions)\n",
    "\n",
    "for i in range(n_class):\n",
    "    for j in range(n_class):\n",
    "        C_yy[i,j] = float(len(np.where((predictions== i)&(train_labels==j))[0]))/m_train\n",
    "\n",
    "mu_y_train_hat = np.zeros(n_class)\n",
    "for i in range(n_class):\n",
    "    mu_y_train_hat[i] = float(len(np.where(predictions == i)[0]))/m_train\n",
    "\n",
    "# print(mu_y_train)\n",
    "# print(C_yy)\n",
    "# prediction on x_test to estimate mu_y\n",
    "print('\\nTesting on test data to estimate mu_y.')\n",
    "range_test_loader = data.DataLoader(range_test_data, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "predictions, acc, _ = test(base_model, device, range_test_loader)\n",
    "mu_y = np.zeros(n_class)\n",
    "for i in range(n_class):\n",
    "    mu_y[i] = float(len(np.where(predictions == i)[0]))/range_m_test\n",
    "# print(mu_y)\n",
    "\n",
    "rho = compute_3deltaC(n_class, m_train, 0.05)\n",
    "alpha = 0.0001\n",
    "w2 = compute_w_opt(C_yy, mu_y, mu_y_train_hat, alpha * rho)\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(\"Our weight: \", w2)\n",
    "print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "# use original test set to test\n",
    "# All test\n",
    "test_data = data.Subset(raw_data, test_indices)\n",
    "#print(\"test_data size: \", len(test_data))\n",
    "test_labels = raw_data.get_test_label()\n",
    "m_test = raw_data.get_testsize()\n",
    "test_loader = data.DataLoader(test_data, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "# compute the true w\n",
    "true_w = compute_true_w(train_labels, test_labels, n_class, m_train, m_test)\n",
    "tw_tensor[0,:] = torch.tensor(true_w)\n",
    "#print('True w is', true_w)\n",
    "mse2 = np.sum(np.square(true_w - w2))/n_class\n",
    "print('Mean square error of (true_w, w2) = ', mse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data Size,  35000\n",
      "All Test Size, 15000\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 2.377542\n",
      "Train Epoch: 1 [1000/20000 (5%)]\tLoss: 0.022703\n",
      "Train Epoch: 1 [2000/20000 (10%)]\tLoss: 0.055659\n",
      "Train Epoch: 1 [3000/20000 (15%)]\tLoss: 0.086093\n",
      "Train Epoch: 1 [4000/20000 (20%)]\tLoss: 0.012160\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.000248\n",
      "Train Epoch: 1 [6000/20000 (30%)]\tLoss: 0.033621\n",
      "Train Epoch: 1 [7000/20000 (35%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [8000/20000 (40%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 1.750118\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.025743\n",
      "Train Epoch: 1 [11000/20000 (55%)]\tLoss: 0.434032\n",
      "Train Epoch: 1 [12000/20000 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [13000/20000 (65%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [14000/20000 (70%)]\tLoss: 2.362927\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.000284\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.119204\n",
      "Train Epoch: 1 [17000/20000 (85%)]\tLoss: 0.000010\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 0.016880\n",
      "Train Epoch: 1 [19000/20000 (95%)]\tLoss: 0.000000\n",
      "Finish training for base model\n"
     ]
    }
   ],
   "source": [
    "# saparate into training and testing\n",
    "m = len(raw_data) # all data\n",
    "print(\"All Data Size, \", m)\n",
    "\n",
    "m_test = raw_data.get_testsize() # all test data\n",
    "print('All Test Size,', m_test)\n",
    "test_indices = range(m_test)\n",
    "\n",
    "test_data = data.Subset(raw_data, range(m_test)) \n",
    "test_labels = raw_data.get_test_label()\n",
    "test_loader = data.DataLoader(test_data, batch_size=1, shuffle=True, **kwargs)\n",
    "\n",
    "train_data = data.Subset(raw_data, range(m_test, m)) # all train data (all data - all test data)\n",
    "train_labels = raw_data.get_train_label()\n",
    "train_loader = data.DataLoader(train_data, batch_size=1, shuffle=True, **kwargs)\n",
    "\n",
    "# train the base model\n",
    "D_in = 784\n",
    "base_model = Net(D_in, 256, 10)\n",
    "base_model = base_model.to(device)\n",
    "#model = ResNet18(**kwargs).to(device)#ConvNet().to(device)\n",
    "optimizer = optim.SGD(base_model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
    "for epoch in range(1, epochs_training + 1):\n",
    "    train(base_model, device, train_loader, optimizer, epoch)\n",
    "print(\"Finish training for base model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_size = 20\n",
    "# prediction, accuracy, test_loss, uncertainty_queue = test_query(base_model, device, test_loader, query_size)\n",
    "# extra_data = []\n",
    "# for item in uncertainty_queue:\n",
    "#     data = item[0]\n",
    "#     target = item[1]\n",
    "#     extra_data.append((data, target))\n",
    "# w2_tensor = torch.from_numpy(w2).float()\n",
    "# train_al(base_model, device, train_loader, extra_data, optimizer, epoch, w2_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction, accuracy, test_loss, uncertainty_queue = test_query(base_model, device, test_loader, query_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query_size = 20\n",
    "# #prediction, accuracy, test_loss, uncertainty_queue = test_query(base_model, device, test_loader, query_size)\n",
    "# extra_data = []\n",
    "# for item in uncertainty_queue:\n",
    "#     data = item[0]\n",
    "#     target = item[1]\n",
    "#     extra_data.append((data, target))\n",
    "# w2_tensor = torch.from_numpy(w2).float()\n",
    "# train_al(base_model, device, train_loader, extra_data, optimizer, epoch, w2_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prediction, accuracy, test_loss, uncertainty_queue = test_query(base_model, device, test_loader, query_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # test 1\n",
    "# query_size = 20\n",
    "# iterations = 5\n",
    "# for i in range(iterations):\n",
    "#     print(\"Current iteration = \", i)\n",
    "#     prediction, accuracy, test_loss, uncertainty_queue = test_query(base_model, device, test_loader, query_size)\n",
    "#     extra_data = []\n",
    "#     for item in uncertainty_queue:\n",
    "#         data = item[0]\n",
    "#         target = item[1]\n",
    "#         extra_data.append((data, target))\n",
    "#     w2_tensor = torch.from_numpy(w2).float()\n",
    "#     train_al(base_model, device, train_loader, extra_data, optimizer, epoch, w2_tensor)\n",
    "    \n",
    "# prediction, accuracy, test_loss, uncertainty_queue = test_query(base_model, device, test_loader, query_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration =  0\n",
      "Average loss: 0.3782, Accuracy: 13633/15000 (90.88667%)\n",
      "\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [1000/20000 (5%)]\tLoss: 0.000323\n",
      "Train Epoch: 1 [2000/20000 (10%)]\tLoss: 0.000572\n",
      "Train Epoch: 1 [3000/20000 (15%)]\tLoss: 7.098568\n",
      "Train Epoch: 1 [4000/20000 (20%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.000147\n",
      "Train Epoch: 1 [6000/20000 (30%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [7000/20000 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8000/20000 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 0.000328\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.002885\n",
      "Train Epoch: 1 [11000/20000 (55%)]\tLoss: 0.683246\n",
      "Train Epoch: 1 [12000/20000 (60%)]\tLoss: 0.000029\n",
      "Train Epoch: 1 [13000/20000 (65%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [14000/20000 (70%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.000101\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 1.070185\n",
      "Train Epoch: 1 [17000/20000 (85%)]\tLoss: 0.000052\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19000/20000 (95%)]\tLoss: 0.008259\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000145\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000010\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000987\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.029669\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000034\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.186485\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000010\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000091\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000021\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000075\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000054\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.031132\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000897\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000010\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.005915\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000115\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000019\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000428\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.017674\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.006310\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.003698\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 4.283345\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.034298\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.058875\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000144\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000050\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.321475\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 10.527332\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 2.514956\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.001715\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000156\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.001853\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000010\n",
      "Current iteration =  1\n",
      "Average loss: 0.3783, Accuracy: 13476/15000 (89.84000%)\n",
      "\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.000458\n",
      "Train Epoch: 1 [1000/20000 (5%)]\tLoss: 5.014304\n",
      "Train Epoch: 1 [2000/20000 (10%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [3000/20000 (15%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [4000/20000 (20%)]\tLoss: 0.005802\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.000020\n",
      "Train Epoch: 1 [6000/20000 (30%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [7000/20000 (35%)]\tLoss: 0.556749\n",
      "Train Epoch: 1 [8000/20000 (40%)]\tLoss: 0.000034\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 0.001035\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [11000/20000 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12000/20000 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [13000/20000 (65%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [14000/20000 (70%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 1.376212\n",
      "Train Epoch: 1 [17000/20000 (85%)]\tLoss: 0.016767\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19000/20000 (95%)]\tLoss: 0.010548\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.253753\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000118\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.007996\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.486679\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000030\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000302\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.006389\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000088\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.251643\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000184\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000026\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.004596\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000212\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.015230\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.388853\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.162226\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.003982\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000072\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.100703\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.889215\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.039323\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000029\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000903\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000027\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.025810\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.296362\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000589\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000010\n",
      "Current iteration =  2\n",
      "Average loss: 0.2966, Accuracy: 13920/15000 (92.80000%)\n",
      "\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.000011\n",
      "Train Epoch: 1 [1000/20000 (5%)]\tLoss: 0.000024\n",
      "Train Epoch: 1 [2000/20000 (10%)]\tLoss: 0.000029\n",
      "Train Epoch: 1 [3000/20000 (15%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [4000/20000 (20%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [6000/20000 (30%)]\tLoss: 0.000285\n",
      "Train Epoch: 1 [7000/20000 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8000/20000 (40%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [11000/20000 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12000/20000 (60%)]\tLoss: 0.003599\n",
      "Train Epoch: 1 [13000/20000 (65%)]\tLoss: 0.014064\n",
      "Train Epoch: 1 [14000/20000 (70%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.227266\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.082985\n",
      "Train Epoch: 1 [17000/20000 (85%)]\tLoss: 1.950789\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19000/20000 (95%)]\tLoss: 0.000010\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000088\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000135\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 5.017806\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000013\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.007284\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000020\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000072\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.036996\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000272\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.001906\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 2.202750\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Current iteration =  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.5648, Accuracy: 13414/15000 (89.42667%)\n",
      "\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [1000/20000 (5%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [2000/20000 (10%)]\tLoss: 0.071839\n",
      "Train Epoch: 1 [3000/20000 (15%)]\tLoss: 0.000555\n",
      "Train Epoch: 1 [4000/20000 (20%)]\tLoss: 5.213548\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.002249\n",
      "Train Epoch: 1 [6000/20000 (30%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [7000/20000 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8000/20000 (40%)]\tLoss: 0.000023\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.002155\n",
      "Train Epoch: 1 [11000/20000 (55%)]\tLoss: 0.016938\n",
      "Train Epoch: 1 [12000/20000 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [13000/20000 (65%)]\tLoss: 0.084923\n",
      "Train Epoch: 1 [14000/20000 (70%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.000233\n",
      "Train Epoch: 1 [17000/20000 (85%)]\tLoss: 1.312225\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 0.065094\n",
      "Train Epoch: 1 [19000/20000 (95%)]\tLoss: 0.000481\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.014676\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.004914\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.022088\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.138999\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 3.316878\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.001365\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.004657\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.004370\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000200\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 3.306213\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.073058\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000084\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.006152\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000015\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.303898\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.115093\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.375594\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.015212\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.005731\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000876\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.001907\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000151\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.015205\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.053627\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000040\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.228352\n",
      "Current iteration =  4\n",
      "Average loss: 0.3161, Accuracy: 13825/15000 (92.16667%)\n",
      "\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.000012\n",
      "Train Epoch: 1 [1000/20000 (5%)]\tLoss: 0.000269\n",
      "Train Epoch: 1 [2000/20000 (10%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [3000/20000 (15%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [4000/20000 (20%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [6000/20000 (30%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [7000/20000 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8000/20000 (40%)]\tLoss: 0.001410\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 0.032409\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [11000/20000 (55%)]\tLoss: 0.004299\n",
      "Train Epoch: 1 [12000/20000 (60%)]\tLoss: 2.246447\n",
      "Train Epoch: 1 [13000/20000 (65%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [14000/20000 (70%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.760744\n",
      "Train Epoch: 1 [17000/20000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 0.004116\n",
      "Train Epoch: 1 [19000/20000 (95%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.002677\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 3.540955\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000027\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.006384\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000031\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000029\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 2.087712\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.430708\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.091190\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000188\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000066\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000007\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.357339\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 5.028669\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.001842\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.058074\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000459\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.020108\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.008348\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.705554\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000877\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000132\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.807701\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.076569\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Current iteration =  5\n",
      "Average loss: 0.3997, Accuracy: 13785/15000 (91.90000%)\n",
      "\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.000111\n",
      "Train Epoch: 1 [1000/20000 (5%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [2000/20000 (10%)]\tLoss: 0.000150\n",
      "Train Epoch: 1 [3000/20000 (15%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [4000/20000 (20%)]\tLoss: 0.001278\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.001835\n",
      "Train Epoch: 1 [6000/20000 (30%)]\tLoss: 0.000116\n",
      "Train Epoch: 1 [7000/20000 (35%)]\tLoss: 0.332659\n",
      "Train Epoch: 1 [8000/20000 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.000063\n",
      "Train Epoch: 1 [11000/20000 (55%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [12000/20000 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [13000/20000 (65%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [14000/20000 (70%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.001036\n",
      "Train Epoch: 1 [17000/20000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 0.000122\n",
      "Train Epoch: 1 [19000/20000 (95%)]\tLoss: 0.000023\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.175094\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000470\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.007256\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.005981\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000108\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.249873\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000305\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 4.563540\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000846\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000013\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000187\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000154\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.021947\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.002103\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000049\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000080\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.001981\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.027878\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.005964\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.010595\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.001999\n",
      "Current iteration =  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.3201, Accuracy: 14018/15000 (93.45333%)\n",
      "\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [1000/20000 (5%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [2000/20000 (10%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [3000/20000 (15%)]\tLoss: 0.388343\n",
      "Train Epoch: 1 [4000/20000 (20%)]\tLoss: 0.007371\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [6000/20000 (30%)]\tLoss: 0.604896\n",
      "Train Epoch: 1 [7000/20000 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8000/20000 (40%)]\tLoss: 0.008556\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 0.000145\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [11000/20000 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12000/20000 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [13000/20000 (65%)]\tLoss: 0.000053\n",
      "Train Epoch: 1 [14000/20000 (70%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.054120\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [17000/20000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 0.000093\n",
      "Train Epoch: 1 [19000/20000 (95%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.555460\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.436288\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.006367\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000016\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.003010\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000472\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.003917\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.001308\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000019\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 2.779149\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.034192\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000086\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.004036\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000174\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000199\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.838255\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.070373\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.136034\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000216\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000302\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000016\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.003963\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.344008\n",
      "Current iteration =  7\n",
      "Average loss: 0.4211, Accuracy: 13593/15000 (90.62000%)\n",
      "\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.001268\n",
      "Train Epoch: 1 [1000/20000 (5%)]\tLoss: 0.292661\n",
      "Train Epoch: 1 [2000/20000 (10%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [3000/20000 (15%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [4000/20000 (20%)]\tLoss: 0.076422\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [6000/20000 (30%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [7000/20000 (35%)]\tLoss: 0.364957\n",
      "Train Epoch: 1 [8000/20000 (40%)]\tLoss: 0.591431\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 0.113593\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [11000/20000 (55%)]\tLoss: 0.097369\n",
      "Train Epoch: 1 [12000/20000 (60%)]\tLoss: 0.004147\n",
      "Train Epoch: 1 [13000/20000 (65%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [14000/20000 (70%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.702330\n",
      "Train Epoch: 1 [17000/20000 (85%)]\tLoss: 4.423662\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [19000/20000 (95%)]\tLoss: 0.086355\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.105359\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.006676\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 4.191212\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.059442\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000661\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.007883\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000085\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.002966\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.090773\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.033683\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000153\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.010775\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000293\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 4.377499\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000252\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000337\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.003876\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.002394\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.004446\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.020852\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.012744\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000034\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 3.234716\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000145\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000172\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.001221\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.005053\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.154457\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000632\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.165094\n",
      "Current iteration =  8\n",
      "Average loss: 0.2914, Accuracy: 13885/15000 (92.56667%)\n",
      "\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.903491\n",
      "Train Epoch: 1 [1000/20000 (5%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [2000/20000 (10%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [3000/20000 (15%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [4000/20000 (20%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [6000/20000 (30%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [7000/20000 (35%)]\tLoss: 0.001469\n",
      "Train Epoch: 1 [8000/20000 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 0.018775\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [11000/20000 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [12000/20000 (60%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [13000/20000 (65%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [14000/20000 (70%)]\tLoss: 0.000037\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.011218\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [17000/20000 (85%)]\tLoss: 0.000129\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 0.074450\n",
      "Train Epoch: 1 [19000/20000 (95%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.022787\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000253\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.078644\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.108782\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.020301\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 1.075460\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000040\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 2.249230\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.026157\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.002052\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 9.336567\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000108\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000188\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000022\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.002751\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.010616\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000134\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.001448\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000470\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Current iteration =  9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.2660, Accuracy: 14100/15000 (94.00000%)\n",
      "\n",
      "Train Epoch: 1 [0/20000 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [1000/20000 (5%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [2000/20000 (10%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [3000/20000 (15%)]\tLoss: 0.000051\n",
      "Train Epoch: 1 [4000/20000 (20%)]\tLoss: 1.279104\n",
      "Train Epoch: 1 [5000/20000 (25%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [6000/20000 (30%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [7000/20000 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [8000/20000 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [9000/20000 (45%)]\tLoss: 0.000103\n",
      "Train Epoch: 1 [10000/20000 (50%)]\tLoss: 0.000013\n",
      "Train Epoch: 1 [11000/20000 (55%)]\tLoss: 13.953465\n",
      "Train Epoch: 1 [12000/20000 (60%)]\tLoss: 0.000010\n",
      "Train Epoch: 1 [13000/20000 (65%)]\tLoss: 0.414015\n",
      "Train Epoch: 1 [14000/20000 (70%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [15000/20000 (75%)]\tLoss: 0.012470\n",
      "Train Epoch: 1 [16000/20000 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [17000/20000 (85%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [18000/20000 (90%)]\tLoss: 0.000262\n",
      "Train Epoch: 1 [19000/20000 (95%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000141\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.003612\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000011\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.645795\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000013\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000008\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.031106\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000450\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.010686\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000496\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000013\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 2.924779\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000021\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.063643\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000137\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.010280\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000164\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.002026\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000112\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000664\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000042\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [0/50 (0%)]\tLoss: 0.000131\n",
      "Average loss: 0.2326, Accuracy: 14150/15000 (94.33333%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_size = 50\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    print(\"Current iteration = \", i)\n",
    "    prediction, accuracy, test_loss, uncertainty_queue = test_query(base_model, device, test_loader, query_size)\n",
    "    extra_data = []\n",
    "    for item in uncertainty_queue:\n",
    "        data = item[0]\n",
    "        target = item[1]\n",
    "        extra_data.append((data, target))\n",
    "    w2_tensor = torch.from_numpy(w2).float()\n",
    "    train_al(base_model, device, train_loader, extra_data, optimizer, epoch, w2_tensor)\n",
    "    \n",
    "prediction, accuracy, test_loss, uncertainty_queue = test_query(base_model, device, test_loader, query_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
